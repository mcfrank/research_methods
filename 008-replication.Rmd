# Replication {#replication}


Reproducibility is a major problem [in psychology](http://www.nature.com/news/first-results-from-psychology-s-largest-reproducibility-test-1.17433) and [elsewhere](http://www.nature.com/news/reproducibility-1.17552). Much of the published literature is not solid enough to build on: [experiences from my class](http://babieslearninglanguage.blogspot.com/2015/03/estimating-preplication-in-practical.html) suggest that students can get interesting stuff to work about half the time, at best. The [recent findings of the reproducibility project](http://www.theatlantic.com/health/archive/2015/08/psychology-studies-reliability-reproducability-nosek/402466/) only add to this impression.* And awareness has been growing about all kinds of potential problems for reproducibility, including p-hacking, file-drawer effects, and deeper issues in the frequentist data analysis tools many of us were originally trained on. What we should do about this problem?  
  
Many people advocate dramatic changes to our day-to-day scientific practices. While I believe deeply in some of these changes – [open practices](http://babieslearninglanguage.blogspot.com/2014/04/data-analysis-one-step-deeper.html) being [one example](http://babieslearninglanguage.blogspot.com/2014/09/sharing-research-using-rmarkdown.html) – I also worry that some recommendations will hinder the process of normal science. I'm what you might call a "reproducibility moderate." A moderate acknowledges the problem, but believes that the solutions should not be too radical. Instead, solutions should be chosen to conserve the best parts of our current practice.  
  
Here are my thoughts on three popular proposed solutions to the reproducibility crisis: preregistration, publication of null results, and Bayesian statistics. In each case, I believe these techniques should be part of our scientific arsenal – but adopting them wholesale would cause more problems than it would fix.  
  
**Pre-registration.** Pre-registering a study is an important technique for removing analytic degrees of freedom. But it also ties the analysts's hands in ways that can be cumbersome and unnecessary early in a research program, where analytic freedom is critical for making sense of the data (the trick is just not to publish those exploratory analyses as though they are confirmatory). [As I've argued](http://babieslearninglanguage.blogspot.com/2013/07/thoughts-on-preregistration.html), preregistration is a great tool to have in your arsenal for large-scale or one-off studies. In cases where subsequent replications are difficult or overly costly, prereg allows you to have confidence in your analyses. But in cases where you can run a sequence of studies that build on one another, each replicating the key finding and using the same analysis strategy, you don't need to pre-register because your previous work naturally constrains your analysis. So: rather than running more one-off studies but preregistering them, we should be doing more cumulative, sequential work where – for the most part – preregistration isn't needed.  
  
**Publication of null findings.** File drawer biases – where negative results are not published and so effect sizes are inflated across a literature – are a real problem, especially in controversial areas. But the solution is not to publish everything, willy-nilly! Publishing a paper, even a short one or a preprint, is a lot of work. The time you spend writing up null results is time you are not doing new studies. What we need is [thoughtful consideration of when it is ethical to suppress a result, and when there is a clear need to publish](http://babieslearninglanguage.blogspot.com/2014/11/musings-on-file-drawer-effect.html).  
  
**Bayesian statistics.** Frequentist statistical methods have deep conceptual flaws and are broken in any number of ways. But they can still be a useful tool for quantifying our uncertainty about data, and a wholesale abandonment of them in favor of Bayesian stats ([or even worse, nothing!](http://www.nature.com/news/psychology-journal-bans-p-values-1.17001)) risks several negative consequences. First, having a uniform statistical analysis paradigm facilitates evaluation of results. You don't have to be an expert to understand someone's ANOVA analysis. But if everyone uses one-off graphical models (as great as they are), then there are many mistakes we will never catch due to the complexity of the models. Second, the tools for Bayesian data analysis are getting better quickly, but they are nowhere near as easy to use as the frequentist ones. To pick on one system, as an experienced modeler, I love working with [Stan](http://mc-stan.org/). But until it stops crashing my R session, I will not recommend it as a tool for first-year graduate stats. In the mean time, I favor the [Cumming solution](http://pss.sagepub.com/content/early/2013/11/07/0956797613504966): A more gentle move towards confidence intervals, judicious use of effect size, and a decrease in reliance on inferences from individual instances of _p < .05._  
  
Sometimes it looks like we've polarized into two groups: replicators and everyone else. This is crazy! Who wants to spend an entire career replicating other people's work, or even your own? Instead, replication needs to be part of our scientific process more generally. It needs to be a first step, where we build on pre-existing work, and a last step, where we confirm our findings prior to publication. But the steps in the middle – where you do the real discovery – are important as well. If we focus only on those first and last steps and make our recommendations in light of them alone, we forget the basic practice of science.  
  
\-\-\-\-  
\* I'm one of many, many authors of that project, having helped to contribute four replication projects from my graduate class.

## Did It Replicate? 

Psychology is in the middle of a sea change in its attitudes towards direct replication. Despite their value in providing evidence for the reliability of a particular experimental finding, [incentives for direct replications have typically been limited](https://arxiv.org/pdf/1205.4251.pdf). Increasingly, however, [journals](https://twitter.com/jennrichler/status/460431447544061952) and [funding agencies](http://www.nsf.gov/sbe/AC_Materials/SBE_Robust_and_Reliable_Research_Report.pdf) now increasingly value these sorts of efforts. One major challenge, however, has been evaluating the success of direct replications studies. In short, how do we know if the finding is the same?  
  
There has been limited consensus on this issue, so many projects have used a diversity of methods. The [RP:P 100-study replication project](http://science.sciencemag.org/content/349/6251/aac4716), reports several indicators of replication success, including 1) the statistical significance of the replication, 2) whether the original effect size lies within the confidence interval of the replication, 3) the relationship between the original and replication effect size, 4) the meta-analytic estimate of effect size combining both, and 5) a subjective assessment of replication by the team. Mostly these indicators hung together, though there were numerical differences.  
  
Several of these criteria are flawed from a technical perspective. As Uri Simonsohn points out in his ["Small Telescopes" paper](http://pss.sagepub.com/content/26/5/559), as the power of the replication study goes to infinity, the replication will _always_ be statistically significant, even if it's finding a very small effect that's quite different from the original. And similarly, as N in the original study goes to zero (if it's very underpowered), it gets harder and harder to differentiate its effect size from any other, because of its wide confidence interval. So both statistical significance of the replication and comparison of effect sizes have notable flaws.*  
  
In addition, all this trouble is just for a single effect. In fact, one weakness of RP:P was that researchers were forced to choose just a single effect size as the key analysis in the original study. If you start looking at an experiment that has multiple important analyses, the situation gets way worse. Consider a simple 2x2 factorial design: Even if the key test identified by a replicator is the interaction, if the replication study fails to see a main effect or sees a new, un-predicted main effect, those findings might lead someone to say that the replication result was different than the original. And in practice it's even more complicated than that because sometimes it's not straightforward to figure out whether it was the main effect or the interaction the authors cared about (or maybe it was both). Students in [my class](http://psych254.stanford.edu/) routinely struggle to find the key effect that they should focus on in their replication projects.  
  
Recently we had [a case](http://babieslearninglanguage.blogspot.com/2014/11/is-mind-reading-automatic-replication.html) that was yet more confounding than this. We did a direct replication of an influential paper and found that we were able to reproduce every single one of the statistical tests. The only issue was that we _also_ found another significant result where the authors' theory would predict a null effect or even an effect in the opposite direction. (We were studying theory of mind reasoning and we found that participants' responses were slower not only when the state of the world was _incongruent_ with their and others' beliefs, but also when it was _congruent_ with belief information). In this case, it was _only_ the theoretical interpretation that allowed us to argue that our "successful replication" was in fact inconsistent with the authors' theory.  
  
I think this case illustrates a broader generalization, namely that statistical methods for assessing replication success need to be considered as secondary to the theoretical interpretation of the result. Instead, I propose that:  

> _The primary measure of whether a replication result is congruent with the original finding is whether it provides support for the theoretical interpretation given to the original.  _

And as in my discussion of publication bias, I think [the key test is adversarial](http://babieslearninglanguage.blogspot.com/2014/11/musings-on-file-drawer-effect.html). In other words, a replication is unsuccessful if a _knowledgeable and adversarial reviewer_ could reasonably argue that the new data fail to support the interpretation.  

  

Note that statistical and theoretical criteria may often line up in simple cases. If the original study was an RCT of an intervention, then the key theoretical interpretation was essentially captured by the effect size, and so the relationship between the two effect sizes is important. But in other cases, the theoretical interpretation may hinge on the reliability and direction of the effect, not its magnitude; in those cases, the theoretical test gives the right answer while the statistics may not. And similarly, in the case I described above, the theoretical test gives the right answer – our data didn't support the original theory even though the statistics all lined up.  
  
This argument apparently puts me in an odd position, because it seems like I'm advocating for giving up on an important family of quantitative approaches to reproducibility. In particular, the effect-size estimation approach to reproducibility emerges from the tradition of [statistical meta-analysis](https://en.wikipedia.org/wiki/Meta-analysis). And meta-analysis is just about as good as it gets in terms of aggregating data across multiple studies right now. So is this an argument for vagueness?  
  
No. The key point that emerges from this set of ideas is instead that the precision of the original theoretical specification is what governs whether a replication is successful or not. If the original theory is vague, it's simply hard to tell whether what you saw gives support to it, and all the statistics in the world won't really help. (This is of course the problem in all the discussion of context-sensitivity in replication). In contrast, if the original theory is precisely specified, it's very easy to assess support.  
  
In other words, rather than arguing for a vaguer definition of replication success, what I'm instead arguing for is _more precise theories_. Replication is only well-defined if we know what we're looking for. The tools of meta-analysis provide a theory-neutral fix for class of single effect statistics (think of the effect size for the RCT). But once we get beyond the light shed by that small lamp post, theory is going to be the only way we find our keys.  
  
\-\-\-  
\* Simonsohn proposes another – perhaps more promising – criterion for distinguishing effect sizes that I won't go into here because it's limited to the single-effect domain.